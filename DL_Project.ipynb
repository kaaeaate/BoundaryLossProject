{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FCL0wP2-E_rI"
   },
   "outputs": [],
   "source": [
    "# import gc\n",
    "# import torch\n",
    "# gc.collect()\n",
    "# torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gqYORxWKPku1",
    "outputId": "1cc3cb43-232b-46ac-f6d0-559efac30d99"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 28 08:45:49 2021       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 465.19.01    Driver Version: 460.32.03    CUDA Version: 11.2     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n",
      "| N/A   51C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|  No running processes found                                                 |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "L9efFVZ7vN5e"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from pathlib import Path\n",
    "import torchvision\n",
    "import torch\n",
    "from torchvision import datasets, transforms, models\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from PIL import Image\n",
    "import random\n",
    "import cv2\n",
    "\n",
    "import pandas as pd\n",
    "import shutil\n",
    "\n",
    "from scipy.ndimage import distance_transform_edt as eucl_distance\n",
    "from typing import Any, Callable, Iterable, List, Set, Tuple, TypeVar, Union, cast\n",
    "from functools import partial, reduce\n",
    "from operator import itemgetter, mul\n",
    "from torch import Tensor, einsum"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hlBvkEp-XDr1"
   },
   "source": [
    "### Please enter the parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "-Jy8IfLXXC60"
   },
   "outputs": [],
   "source": [
    "# Dataset options:\n",
    "#    - Hippocampus-2D\n",
    "#    - Heart-2D\n",
    "#    - Heart-2D_onlyimage\n",
    "dataset = 'Heart-2D_onlyimage'\n",
    "\n",
    "# Model options:\n",
    "#    - U-Net\n",
    "#    - Attention_U-Net\n",
    "Model = 'U-Net'\n",
    "\n",
    "# Loss options:\n",
    "#    - GeneralizedDice\n",
    "#    - CrossEntropy\n",
    "#    - FocalLoss\n",
    "Loss = 'CrossEntropy'\n",
    "\n",
    "# Boundary_loss options:\n",
    "#    - True\n",
    "#    - False\n",
    "Boundary_option = 'True'\n",
    "\n",
    "n_epochs = 50\n",
    "\n",
    "ALPHA = 0.01"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jMNKH21dXhj4",
    "outputId": "09519967-8fad-4587-90fd-eff1b9095380"
   },
   "outputs": [],
   "source": [
    "if dataset == 'Hippocampus-2D':\n",
    "    path = './Hippocampus-2D'\n",
    "    K = 3\n",
    "    # %cd '/content'\n",
    "    ! gdown --id '1GyTS7KzudC9glm0KZr4qyU5Txfz2PI5c'\n",
    "    ! unzip 'Hippocampus-2D.zip'\n",
    "    path = 'Hippocampus-2D'\n",
    "    batch_size = 8\n",
    "\n",
    "elif dataset == 'Heart-2D':\n",
    "    path = './Heart-2D'\n",
    "    K = 2\n",
    "    # %cd '/content'\n",
    "    ! gdown --id '1VLE1H7IHhVaf04PzVV5yHXstey1ecz5L'\n",
    "    ! unzip 'Heart-2D.zip'\n",
    "    path = 'Heart-2D'\n",
    "    batch_size = 8\n",
    "\n",
    "elif dataset == 'Heart-2D_onlyimage':\n",
    "    path = './Heart-2D_onlyimage'\n",
    "    K = 2\n",
    "    # %cd '/content'\n",
    "    ! gdown --id '1wlXQlhQbpOf6T-su7e3X9vc-Xj_6kc_J'\n",
    "    ! unzip 'Heart-2D_onlyimage.zip'\n",
    "    path = 'Heart-2D_onlyimage'\n",
    "    batch_size = 8\n",
    "\n",
    "else:\n",
    "    print('Such a dataset does not exist for these experiments')\n",
    "\n",
    "set_classes = list(range(K))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX6Hw7zh1Rhj"
   },
   "source": [
    "### Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "NcznY6lDG5jJ"
   },
   "outputs": [],
   "source": [
    "### asserts \n",
    "\n",
    "def sset(a, sub):\n",
    "    return uniq(a).issubset(sub)  \n",
    "\n",
    "def uniq(a):\n",
    "    return set(torch.unique(a.cpu()).numpy())\n",
    "\n",
    "def one_hot(t, axis=1):\n",
    "    return simplex(t, axis) and sset(t, set_classes)  \n",
    "\n",
    "def simplex(t, axis=1):                                                          \n",
    "    _sum = cast(Tensor, t.sum(axis).type(torch.float32))                        \n",
    "    _ones = torch.ones_like(_sum, dtype=torch.float32)\n",
    "    return torch.allclose(_sum, _ones)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "qBQ4HDNAHPI7"
   },
   "outputs": [],
   "source": [
    "class Transforms():\n",
    "  def __init__(self, resolution, K):\n",
    "\n",
    "      self.K = K\n",
    "      self.resolution = resolution\n",
    "      self.encods = Encoding()\n",
    "\n",
    "      self.image_transform = transforms.Compose([\n",
    "            lambda img: cv2.cvtColor(img, cv2.COLOR_BGR2GRAY), \n",
    "            lambda img: img[np.newaxis, ...], \n",
    "            lambda img: img / 255,  \n",
    "            lambda img: torch.tensor(img, dtype=torch.float32)\n",
    "      ])\n",
    "\n",
    "      self.mask_transform = transforms.Compose([\n",
    "            lambda img: img[...],\n",
    "            lambda img: torch.tensor(img, dtype=torch.int64)[None, ...],\n",
    "            partial(self.encods.one_hot_encoding, K=self.K),\n",
    "            itemgetter(0)  \n",
    "      ])\n",
    "\n",
    "\n",
    "      self.dist_map_transform = transforms.Compose([\n",
    "            self.mask_transform,\n",
    "            lambda img: img.cpu().numpy(),\n",
    "            partial(self.encods.dist_encoding, resolution=self.resolution),                   \n",
    "            lambda img: torch.tensor(img, dtype=torch.float32) \n",
    "      ])\n",
    "     \n",
    "\n",
    "  def forward(self, img, transform_type):\n",
    "        if transform_type == 'img':\n",
    "          return self.image_transform(img)\n",
    "\n",
    "        if transform_type == 'mask':\n",
    "          return self.mask_transform(img)\n",
    "\n",
    "        if transform_type == 'dist_map':\n",
    "          return self.dist_map_transform(img)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "id": "PO1d3mvUJKym"
   },
   "outputs": [],
   "source": [
    "class Encoding():\n",
    "  def __init__(self):\n",
    "      super().__init__()\n",
    "\n",
    "  def one_hot_encoding(self, mask_input, K):\n",
    "    assert sset(mask_input, list(range(K))), (uniq(mask_input), K)\n",
    "\n",
    "    b, *img_shape = mask_input.shape  \n",
    "    device = mask_input.device\n",
    "\n",
    "    output = torch.zeros((b, K, *img_shape), dtype=torch.int64, device=device).scatter_(1, mask_input[:, None, ...], 1)\n",
    "    \n",
    "    assert output.shape == (b, K, *img_shape)\n",
    "    assert one_hot(output)\n",
    "    return output \n",
    "\n",
    "  def dist_encoding(self, mask, resolution, dtype=None):                                  \n",
    "    assert one_hot(torch.tensor(mask), axis=0)                                   \n",
    "    K = len(mask)                                                                \n",
    "\n",
    "    output = np.zeros_like(mask, dtype=dtype)\n",
    "\n",
    "    for k in range(K):\n",
    "        posmask = mask[k].astype(np.bool)\n",
    "\n",
    "        if posmask.any():                                                      \n",
    "            negmask = ~posmask\n",
    "            output[k] = eucl_distance(negmask, sampling=resolution) * negmask - (eucl_distance(posmask, sampling=resolution) - 1) * posmask\n",
    "            # res1 = cv2.distanceTransform(np.array(negmask, dtype='uint8'), distanceType=cv2.DIST_L2, maskSize=5).astype(np.float32)\n",
    "            # res2 = cv2.distanceTransform(np.array(posmask, dtype='uint8'), distanceType=cv2.DIST_L2, maskSize=5).astype(np.float32)\n",
    "            # output[k] = res1 * negmask - (res2-1) * posmask  \n",
    "\n",
    "    return output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "id": "v-xiu9T9HEB1"
   },
   "outputs": [],
   "source": [
    "class Boundary_Loss():\n",
    "    def __init__(self, idc):\n",
    "        self.idc = idc                                              \n",
    "\n",
    "    def __call__(self, probs, dist_maps):\n",
    "        assert simplex(probs)                                                    \n",
    "        assert not one_hot(dist_maps)\n",
    "\n",
    "        prob = probs[:, self.idc, ...].type(torch.float32)                        \n",
    "        dist_map = dist_maps[:, self.idc, ...].type(torch.float32)\n",
    "\n",
    "        multipled = einsum(\"bkwh,bkwh->bkwh\", prob, dist_map)                           \n",
    "        loss = multipled.mean()                                               \n",
    "        return loss\n",
    "\n",
    "BoundaryLoss = Boundary_Loss                                                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "id": "jEMxZGg09t9k"
   },
   "outputs": [],
   "source": [
    "def plotting(masks, K):\n",
    "  fig, axs = plt.subplots(ncols=K, sharex=True, figsize=(15, 15))\n",
    "  for i in range(K):\n",
    "    axs[i].imshow(masks[i], cmap='seismic', interpolation='none')\n",
    "    axs[i].title.set_text('Class: {}'.format(i))\n",
    "  plt.show()\n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "id": "Ip_oeP2kIbWp"
   },
   "outputs": [],
   "source": [
    "def plot_one_mask(masks, num_class):\n",
    "  final_mask = np.zeros_like(masks[0])\n",
    "  for i in range (num_class):\n",
    "    final_mask[masks[i] > 0.3] = i\n",
    "  \n",
    "  plt.imshow(final_mask, cmap='seismic', interpolation='none')\n",
    "  plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NDZ3O8fvTraC"
   },
   "source": [
    "### Dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "id": "u5Fb-P0wDJUD"
   },
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, images_folder, masks_folder, K):\n",
    "        super(Dataset, self).__init__()\n",
    "        \n",
    "        self.K = K\n",
    "        self.images_folder = images_folder\n",
    "        self.masks_folder = masks_folder\n",
    "        self.images_names = np.sort(os.listdir(images_folder))\n",
    "        self.masks_names = np.sort(os.listdir(masks_folder))\n",
    "        self.transform = Transforms([1, 1], self.K)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images_names)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item_image = cv2.imread(os.path.join(self.images_folder,\n",
    "                                            self.images_names[idx]))   \n",
    "        item_mask = cv2.imread(os.path.join(self.masks_folder,\n",
    "                                              self.masks_names[idx]))\n",
    "        item_mask = cv2.cvtColor(item_mask, cv2.COLOR_BGR2GRAY)\n",
    "        \n",
    "        SEED = np.random.randint(123456789)\n",
    "        if self.transform is not None:\n",
    "            random.seed(SEED)\n",
    "            image_tensor = self.transform.forward(item_image, 'img')     \n",
    "            one_hot_tensor = self.transform.forward(item_mask, 'mask')\n",
    "            dist_map_tensor = self.transform.forward(item_mask, 'dist_map')\n",
    "      \n",
    "\n",
    "        return image_tensor, one_hot_tensor, dist_map_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "Gompcl7MEcYo"
   },
   "outputs": [],
   "source": [
    "train_set = MyDataset(images_folder = path + '/train/img/', \n",
    "                      masks_folder = path + '/train/gt', K=K)\n",
    "\n",
    "\n",
    "val_set = MyDataset(images_folder = path + '/val/img/', \n",
    "                      masks_folder = path + '/val/gt', K=K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "XUckPuRO1Rhl",
    "outputId": "94f8ed21-23b9-4732-9eb9-802149aa4147"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1622\n",
      "1622\n",
      "649\n",
      "649\n"
     ]
    }
   ],
   "source": [
    "print(len(train_set.images_names))\n",
    "print(len(train_set.masks_names))\n",
    "print(len(val_set.images_names))\n",
    "print(len(val_set.masks_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7LHo91txUVrQ"
   },
   "source": [
    "### Import files with losses and models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "5cgeUXfmCciR",
    "outputId": "57c32299-7f4e-48ba-e720-ec462a2f168e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fatal: destination path 'BoundaryLossProject' already exists and is not an empty directory.\n"
     ]
    }
   ],
   "source": [
    "! git clone https://github.com/kaaeaate/BoundaryLossProject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "4uZBmPotfgBy"
   },
   "outputs": [],
   "source": [
    "## If you don't want to clone git repo, you can download needed files with code below\n",
    "\n",
    "# ! gdown --id '1G-qIa21Ouxac0UNRRiO0v-Qo_HB_Oj2f'\n",
    "# ! gdown --id '1YK1LRF9MesidzIZRSGkdshKdiZh2KsQT'\n",
    "# ! gdown --id '16kzaMQteWcjqXbyZLWwNhCdQPzD2FawD'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-s3ip7Tr1Rhl",
    "outputId": "20ad5955-1d0a-4ddc-c6d2-e0783340c154"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'train': 1622, 'val': 649}"
      ]
     },
     "execution_count": 28,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets = {\n",
    "    'train': train_set, 'val': val_set\n",
    "}\n",
    "\n",
    "batch_size_train = batch_size\n",
    "batch_size_val = batch_size_train\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size_train, shuffle=True, num_workers=0),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size_val, shuffle=True, num_workers=0)\n",
    "}\n",
    "\n",
    "dataset_sizes = {\n",
    "    x: len(image_datasets[x]) for x in image_datasets.keys()\n",
    "}\n",
    "\n",
    "dataset_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "0sqsx54T1Rhm",
    "outputId": "2a43bd21-fd59-4e08-f3d6-f4abe6eb5c41"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image shape: torch.Size([8, 1, 128, 128])\n",
      "Mask shape: torch.Size([8, 2, 128, 128])\n",
      "0.0 0.8392157 0.0898413 0.1367945\n",
      "0 1 0.5 0.5\n"
     ]
    }
   ],
   "source": [
    "# See example of images\n",
    "\n",
    "import torchvision.utils\n",
    "\n",
    "def reverse_transform(inp):\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    inp = (inp * 255).astype(np.uint8)\n",
    "    \n",
    "    return inp\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, masks, dist_map = next(iter(dataloaders['train']))\n",
    "\n",
    "print('Image shape:', inputs.shape)\n",
    "print('Mask shape:', masks.shape)\n",
    "for x in [inputs.numpy(), masks.numpy()]:\n",
    "    print(x.min(), x.max(), x.mean(), x.std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 444
    },
    "id": "lnb2I__xmA3C",
    "outputId": "1435bc01-a501-4210-e6da-837eaa06fa0d"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1080x1080 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "batch_number = 2\n",
    "plotting(masks[batch_number], masks.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 268
    },
    "id": "wI2xjSiiHnc3",
    "outputId": "67b118b0-8824-4b73-c147-15b75712fd93"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light",
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_one_mask(masks[batch_number], masks.shape[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iriOuYg01Rhn"
   },
   "source": [
    "# Start the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "KT-4MsHJc7lE"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import copy\n",
    "from collections import defaultdict\n",
    "import torch.nn.functional as F  \n",
    "from BoundaryLossProject.metrics import dice_loss\n",
    "from tqdm.notebook import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "lb9HPToF1Rho"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4PyyAAQdm-nN"
   },
   "source": [
    "### Losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "id": "8LnQqCXmd9RL"
   },
   "outputs": [],
   "source": [
    "class GeneralizedDice():\n",
    "    def __init__(self, idc):\n",
    "        self.idc = idc\n",
    "\n",
    "    def __call__(self, probs, target):\n",
    "        assert simplex(probs) and simplex(target)\n",
    "\n",
    "        pc = probs[:, self.idc, ...].type(torch.float32)\n",
    "        tc = target[:, self.idc, ...].type(torch.float32)\n",
    "\n",
    "        w = 1 / ((einsum(\"bkwh->bk\", tc).type(torch.float32) + 1e-10) ** 2)\n",
    "        intersection = w * einsum(\"bkwh,bkwh->bk\", pc, tc)\n",
    "        union = w * (einsum(\"bkwh->bk\", pc) + einsum(\"bkwh->bk\", tc))\n",
    "\n",
    "        divided = 1 - 2 * (einsum(\"bk->b\", intersection) + 1e-10) / (einsum(\"bk->b\", union) + 1e-10)\n",
    "\n",
    "        loss = divided.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "id": "YwNg-90ShpkY"
   },
   "outputs": [],
   "source": [
    "class CrossEntropy():\n",
    "    def __init__(self, idc):\n",
    "        self.idc: List[int] = idc\n",
    "\n",
    "    def __call__(self, probs, target):\n",
    "        assert simplex(probs) and simplex(target)\n",
    "\n",
    "        log_p = (probs[:, self.idc, ...] + 1e-10).log()\n",
    "        mask = cast(Tensor, target[:, self.idc, ...].type(torch.float32))\n",
    "\n",
    "        loss = - einsum(\"bkwh,bkwh->\", mask, log_p)\n",
    "        loss /= mask.sum() + 1e-10\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "id": "3lnCrUWVh-nV"
   },
   "outputs": [],
   "source": [
    "class FocalLoss():\n",
    "    def __init__(self, idc, gamma=2):\n",
    "        self.idc = idc\n",
    "        self.gamma = gamma\n",
    "\n",
    "    def __call__(self, probs, target):\n",
    "        assert simplex(probs) and simplex(target)\n",
    "\n",
    "        masked_probs = probs[:, self.idc, ...]\n",
    "        log_p = (masked_probs + 1e-10).log()\n",
    "        mask = cast(Tensor, target[:, self.idc, ...].type(torch.float32))\n",
    "\n",
    "        w = (1 - masked_probs)**self.gamma\n",
    "        loss = - einsum(\"bkwh,bkwh,bkwh->\", w, mask, log_p)\n",
    "        loss /= mask.sum() + 1e-10\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "id": "xNLxUBZBNxIm"
   },
   "outputs": [],
   "source": [
    "class DiceLoss():\n",
    "    def __init__(self, idc):\n",
    "        self.idc = idc\n",
    "\n",
    "    def __call__(self, probs, target):\n",
    "        assert simplex(probs) and simplex(target)\n",
    "\n",
    "        pc = probs[:, self.idc, ...].type(torch.float32)\n",
    "        tc = target[:, self.idc, ...].type(torch.float32)\n",
    "\n",
    "        intersection = einsum(\"bcwh,bcwh->bc\", pc, tc)\n",
    "        union = (einsum(\"bkwh->bk\", pc) + einsum(\"bkwh->bk\", tc))\n",
    "\n",
    "        divided = torch.ones_like(intersection) - (2 * intersection + 1e-10) / (union + 1e-10)\n",
    "\n",
    "        loss = divided.mean()\n",
    "\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Q5GsBKy1Y8aC"
   },
   "source": [
    "### Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "id": "Tu4vvP6yNlMs"
   },
   "outputs": [],
   "source": [
    "def meta_dice(sum_str, label, pred, smooth = 1e-8) -> Tensor:\n",
    "    assert label.shape == pred.shape\n",
    "    assert one_hot(label)\n",
    "    assert one_hot(pred)\n",
    "\n",
    "    inter_size = einsum(sum_str, [intersection(label, pred)]).type(torch.float32)\n",
    "    sum_sizes = (einsum(sum_str, [label]) + einsum(sum_str, [pred])).type(torch.float32)\n",
    "\n",
    "    dices: Tensor = (2 * inter_size + smooth) / (sum_sizes + smooth)\n",
    "\n",
    "    return dices\n",
    "\n",
    "\n",
    "dice_coef = partial(meta_dice, \"bk...->bk\")\n",
    "\n",
    "\n",
    "def intersection(a: Tensor, b: Tensor) -> Tensor:\n",
    "    assert a.shape == b.shape\n",
    "    assert sset(a, [0, 1])\n",
    "    assert sset(b, [0, 1])\n",
    "\n",
    "    res = a & b\n",
    "    assert sset(res, [0, 1])\n",
    "\n",
    "    return res\n",
    "\n",
    "\n",
    "def union(a: Tensor, b: Tensor) -> Tensor:\n",
    "    assert a.shape == b.shape\n",
    "    assert sset(a, [0, 1])\n",
    "    assert sset(b, [0, 1])\n",
    "\n",
    "    res = a | b\n",
    "    assert sset(res, [0, 1])\n",
    "\n",
    "    return res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ljm1wYbPnKjt"
   },
   "source": [
    "### Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "id": "9e4qVzyPEbcK"
   },
   "outputs": [],
   "source": [
    "def target_to_one_mask(masks, num_class):\n",
    "  final_mask = torch.zeros_like(masks[0])\n",
    "  for i in range (num_class):\n",
    "    final_mask[masks[i] > 0.5] = i\n",
    "  return final_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "id": "yBaVUI3hfop6"
   },
   "outputs": [],
   "source": [
    "def calc_loss(pred, target, dist_map_label, metrics, Loss, set_classes, K):\n",
    "\n",
    "    if Loss == 'GeneralizedDice':\n",
    "        loss = GeneralizedDice(idc=set_classes)\n",
    "    elif Loss == 'CrossEntropy':\n",
    "        loss = CrossEntropy(idc=set_classes)\n",
    "    elif Loss == 'FocalLoss':  \n",
    "        loss = FocalLoss(idc=set_classes, gamma=2)\n",
    "    else:\n",
    "        print('Such loss is not used in these experiments')\n",
    "\n",
    "    pred_probs = F.softmax(pred, dim=1)\n",
    "    main_loss = loss(pred_probs, target)\n",
    "\n",
    "    if Boundary_option == 'True':\n",
    "        alpha = ALPHA\n",
    "        boundary_loss = BoundaryLoss(idc=set_classes) \n",
    "        bl_loss = boundary_loss(pred_probs, dist_map_label)  \n",
    "        total_loss = main_loss + alpha * bl_loss\n",
    "        metrics['boundary'] += bl_loss.data.cpu().numpy() * target.size(0)\n",
    "    if Boundary_option == 'False':\n",
    "        total_loss = main_loss \n",
    "\n",
    "    dice, dice2 = dice_loss(pred_probs, target)  \n",
    "    metric_dice = DiceLoss(idc=set_classes)\n",
    "    milti_dice = metric_dice(pred_probs, target)\n",
    "\n",
    "    endoder = Encoding()\n",
    "    new_mask = torch.zeros([target.shape[0], target.shape[2], target.shape[3]], dtype=torch.int64)\n",
    "    for b in range(new_mask.shape[0]):\n",
    "        new_mask[b] = target_to_one_mask(target[b], K)\n",
    "    new_target = endoder.one_hot_encoding(new_mask, K)\n",
    "\n",
    "    new_probs = torch.zeros([pred_probs.shape[0], pred_probs.shape[2], pred_probs.shape[3]], dtype=torch.int64)\n",
    "    for b in range(new_probs.shape[0]):\n",
    "        new_probs[b] = target_to_one_mask(pred_probs[b], K)\n",
    "    new_preds = endoder.one_hot_encoding(new_probs, K)\n",
    "\n",
    "    dsc = dice_coef(new_preds, new_target).mean()\n",
    "\n",
    "    metrics['dice_binar'] += dice2.data.cpu().numpy() * target.size(0)\n",
    "    metrics['old_DSC'] += dsc.data.cpu().numpy() * target.size(0)\n",
    "    metrics['DSC'] += (1.- milti_dice.data.cpu().numpy()) * target.size(0)\n",
    "\n",
    "    metrics[Loss] += main_loss.data.cpu().numpy() * target.size(0)\n",
    "    metrics['loss'] += total_loss.data.cpu().numpy() * target.size(0)\n",
    "    \n",
    "    return total_loss\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):    \n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "        \n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "id": "L65onVBQbtTz"
   },
   "outputs": [],
   "source": [
    "models_dir_loss = 'models_loss'\n",
    "models_dir_dsc = 'models_dsc'\n",
    "files_dir = \"times_txt\" \n",
    "logs_base_dir = './logs_' + dataset\n",
    "os.makedirs(logs_base_dir, exist_ok=True)\n",
    "os.makedirs(models_dir_loss, exist_ok=True)\n",
    "os.makedirs(models_dir_dsc, exist_ok=True)\n",
    "os.makedirs(files_dir, exist_ok=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "id": "kvo6lJbFKfRh"
   },
   "outputs": [],
   "source": [
    "def train_model(model, optimizer, scheduler, experiment_name, Loss, set_classes, K, num_epochs, weights):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "    best_dsc = 0.\n",
    "    time_sum = 0.\n",
    "    epoches = []\n",
    "    \n",
    "    writer = SummaryWriter(f'{logs_base_dir}/{experiment_name}')\n",
    "\n",
    "    metrics_dice_train = []\n",
    "    metrics_dice_val = []\n",
    "    for epoch in tqdm(range(num_epochs)):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        since = time.time()\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "                    \n",
    "                model.train()  \n",
    "            else:\n",
    "                model.eval()   \n",
    "\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "            \n",
    "            for inputs, labels, dist_map_label in dataloaders[phase]:\n",
    "\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device) \n",
    "                dist_map_label = dist_map_label.to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    pred = F.softmax(outputs, dim=1) \n",
    "                    loss = calc_loss(outputs, labels, dist_map_label, metrics, Loss, set_classes, K)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            plot_one_mask(labels[0].detach().cpu().numpy(),K)\n",
    "            plot_one_mask(pred[0].detach().cpu().numpy(),K)\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "\n",
    "\n",
    "            epoch_dice2 = metrics['old_DSC'] / epoch_samples\n",
    "            writer.add_scalar(f'old_DSC_{phase}', epoch_dice2.item(), global_step=epoch)\n",
    "\n",
    "            dsc_epoch = metrics['DSC'] / epoch_samples\n",
    "            writer.add_scalar(f'DSC_{phase}', dsc_epoch.item(), global_step=epoch)\n",
    "\n",
    "\n",
    "            main_loss = metrics[Loss] / epoch_samples\n",
    "            writer.add_scalar(f'{Loss}_{phase}', main_loss.item(), global_step=epoch)\n",
    "\n",
    "            if Boundary_option == 'True':\n",
    "                epoch_boundary = metrics['boundary'] / epoch_samples\n",
    "                writer.add_scalar(f'Boundary_value_{phase}', epoch_boundary.item(), global_step=epoch)\n",
    "\n",
    "\n",
    "            epoch_dice = metrics['dice_binar'] / epoch_samples\n",
    "            writer.add_scalar(f'old_DICE_binar_{phase}', epoch_dice.item(), global_step=epoch)\n",
    "\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "            writer.add_scalar(f'Loss_{phase}', epoch_loss.item(), global_step=epoch)\n",
    "\n",
    "            \n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model with loss\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts_loss = copy.deepcopy(model.state_dict())\n",
    "                # load best model weights\n",
    "                model.load_state_dict(best_model_wts_loss) \n",
    "                if weights == 'yes':\n",
    "                    torch.save (model, f\"{models_dir_loss}/loss_model_\" + Model + '_' + '_' + dataset + '_' + Loss + '_' + Boundary_option  + \".pth\")  #### save model\n",
    "\n",
    "            if phase == 'val' and dsc_epoch > best_dsc:\n",
    "                print(\"saving best model with dsc\")\n",
    "                best_dsc = dsc_epoch\n",
    "                best_model_wts_dsc = copy.deepcopy(model.state_dict())\n",
    "                print('Best val DSC: {:4f}'.format(best_dsc))\n",
    "                # load best model weights\n",
    "                model.load_state_dict(best_model_wts_dsc) \n",
    "                if weights == 'yes':\n",
    "                    torch.save (model, f\"{models_dir_dsc}/dsc_model_\" + Model + '_' + '_' + dataset + '_' + Loss + '_' + Boundary_option  + \".pth\")  #### save model\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        time_sum += time_elapsed\n",
    "        file_name = \"time_\" + Model + '_' + '_' + dataset + '_' + Loss + '_' + Boundary_option \n",
    "        with open(f\"{files_dir}/{file_name}.txt\",\"a\") as the_file:\n",
    "            the_file.write('Epoch {}/{}:'.format(epoch, num_epochs - 1) + '  ' + str('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))+'\\n')\n",
    "\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val DSC: {:4f}'.format(best_dsc))\n",
    "\n",
    "    with open(f\"{files_dir}/{file_name}.txt\",\"a\") as the_file:\n",
    "        the_file.write('Total time:' + '  ' + str('{:.0f}m {:.0f}s'.format(time_sum // 60, time_sum % 60))+'\\n')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 838
    },
    "id": "7cVMu2e1CupM",
    "outputId": "ec20c6bc-3bbd-4ee5-fbf2-d6b80649d3cd"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Reusing TensorBoard on port 6006 (pid 128), started 0:13:25 ago. (Use '!kill 128' to kill it.)"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "application/javascript": [
       "\n",
       "        (async () => {\n",
       "            const url = new URL(await google.colab.kernel.proxyPort(6006, {'cache': true}));\n",
       "            url.searchParams.set('tensorboardColab', 'true');\n",
       "            const iframe = document.createElement('iframe');\n",
       "            iframe.src = url;\n",
       "            iframe.setAttribute('width', '100%');\n",
       "            iframe.setAttribute('height', '800');\n",
       "            iframe.setAttribute('frameborder', 0);\n",
       "            document.body.appendChild(iframe);\n",
       "        })();\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.Javascript object>"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%load_ext tensorboard\n",
    "logs_base_dir = './logs_' + dataset\n",
    "%tensorboard --logdir {logs_base_dir}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6DIpNd8pGODw"
   },
   "source": [
    "# Run model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000,
     "referenced_widgets": [
      "edf1a44e65c54604a2883402224bde14",
      "1fad127f6d924292b82497ab8908f044",
      "f17e6b8430b6459a9ce480f2e0caa31e",
      "25d75545ff7447e9a237036628786db2",
      "76d0155cffdd4e48b2dd10099795ab79",
      "9bb86e8d9d684e20bf6be01debc08267",
      "b8347e6cc8e04689b506b6e5b44c6c17",
      "eb576fd071ea41a7b31d24814f9e8ca7"
     ]
    },
    "id": "pJ9C4aEOKi0m",
    "outputId": "7669e835-4f1f-4c75-d80d-e58b4c4eb7d9"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import time\n",
    "import copy\n",
    "from BoundaryLossProject import unet\n",
    "from BoundaryLossProject import  attention_unet\n",
    "import helper\n",
    "from datetime import datetime \n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = K\n",
    "\n",
    "if Model == 'U-Net':\n",
    "    model = unet.UNet(1, num_class).to(device)\n",
    "elif Model == 'Attention_U-Net':\n",
    "    model = attention_unet.AttU_Net(img_ch=1,output_ch=num_class).to(device)\n",
    "else:\n",
    "    print('Your model is not exist')\n",
    "\n",
    "\n",
    "# Optimizer\n",
    "optimizer_ft = optim.Adam(model.parameters(), lr=1e-3)\n",
    "\n",
    "exp_name = datetime.now().isoformat(timespec='minutes') + '_' + Model + '_' + dataset + '_' + Loss + '_' + Boundary_option\n",
    "\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_ft, step_size=25, gamma=0.1)\n",
    "\n",
    "model = train_model(model, optimizer_ft, exp_lr_scheduler, \n",
    "                    experiment_name=exp_name, Loss=Loss, set_classes=set_classes, K=K, num_epochs=n_epochs, weights='yes') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "JVXOiQ4TEXuc"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import helper\n",
    "\n",
    "model.eval()   \n",
    "val_batch = torch.utils.data.DataLoader(val_set,\n",
    "                                            batch_size=4,\n",
    "                                            shuffle=True)\n",
    "        \n",
    "inputs, labels, dist = next(iter(val_batch))\n",
    "inputs = inputs.to(device)\n",
    "labels = labels.to(device)\n",
    "\n",
    "pred = model(inputs)\n",
    "pred = pred.data.cpu().numpy()\n",
    "print(pred.shape)\n",
    "inputs = inputs.cpu()\n",
    "labels = labels.cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PjpCF1wUljAH"
   },
   "outputs": [],
   "source": [
    "i=0\n",
    "j=1\n",
    "plt.imshow(inputs[i][1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IyN10Ox5CzpA"
   },
   "outputs": [],
   "source": [
    "i=1\n",
    "j=1\n",
    "plt.imshow(labels[i][j])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "A6OKvqyPF5AG"
   },
   "outputs": [],
   "source": [
    "pred = pred >0.3\n",
    "plt.imshow(pred[i][j])\n",
    "plt.show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sbCKpB5-WQ_T"
   },
   "outputs": [],
   "source": [
    "plot_one_mask(pred[i],K)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qOoBnVZtfj5M"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "DL_Project.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "1fad127f6d924292b82497ab8908f044": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "25d75545ff7447e9a237036628786db2": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_eb576fd071ea41a7b31d24814f9e8ca7",
      "placeholder": "​",
      "style": "IPY_MODEL_b8347e6cc8e04689b506b6e5b44c6c17",
      "value": " 2/50 [02:33&lt;1:01:23, 76.74s/it]"
     }
    },
    "76d0155cffdd4e48b2dd10099795ab79": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    },
    "9bb86e8d9d684e20bf6be01debc08267": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "b8347e6cc8e04689b506b6e5b44c6c17": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "eb576fd071ea41a7b31d24814f9e8ca7": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "edf1a44e65c54604a2883402224bde14": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_f17e6b8430b6459a9ce480f2e0caa31e",
       "IPY_MODEL_25d75545ff7447e9a237036628786db2"
      ],
      "layout": "IPY_MODEL_1fad127f6d924292b82497ab8908f044"
     }
    },
    "f17e6b8430b6459a9ce480f2e0caa31e": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "",
      "description": "  4%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_9bb86e8d9d684e20bf6be01debc08267",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_76d0155cffdd4e48b2dd10099795ab79",
      "value": 2
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
